{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Applied Optimization (TensorFlow)\n",
        "\n",
        "In this lab, we will apply Regularization (L2, Dropout), Optimization (Adam), and Batch Normalization to a CNN for CIFAR-10.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 0: Setup and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load CIFAR-10\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Baseline Model (Likely High Variance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_baseline_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(10))\n",
        "    return model\n",
        "\n",
        "baseline_model = build_baseline_model()\n",
        "baseline_model.compile(optimizer='sgd',\n",
        "                       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "history_base = baseline_model.fit(train_images, train_labels, epochs=10, \n",
        "                                  validation_data=(test_images, test_labels), verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Adding Regularization (L2 + Dropout)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_reg_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001), input_shape=(32, 32, 3)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "    \n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "    \n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(10))\n",
        "    return model\n",
        "\n",
        "reg_model = build_reg_model()\n",
        "reg_model.compile(optimizer='sgd',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "history_reg = reg_model.fit(train_images, train_labels, epochs=10, \n",
        "                            validation_data=(test_images, test_labels), verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Adding Adam Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adam_model = build_reg_model()\n",
        "adam_model.compile(optimizer='adam',\n",
        "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "history_adam = adam_model.fit(train_images, train_labels, epochs=10, \n",
        "                              validation_data=(test_images, test_labels), verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Adding Batch Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_bn_model():\n",
        "    model = models.Sequential()\n",
        "    \n",
        "    model.add(layers.Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "    \n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.Dense(10))\n",
        "    return model\n",
        "\n",
        "bn_model = build_bn_model()\n",
        "bn_model.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "history_bn = bn_model.fit(train_images, train_labels, epochs=10, \n",
        "                          validation_data=(test_images, test_labels), verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Compare Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(history_base.history['val_accuracy'], label='Baseline')\n",
        "plt.plot(history_reg.history['val_accuracy'], label='L2 + Dropout')\n",
        "plt.plot(history_adam.history['val_accuracy'], label='L2 + Dropout + Adam')\n",
        "plt.plot(history_bn.history['val_accuracy'], label='Full Model (BN + Adam)')\n",
        "plt.title('Validation Accuracy Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}