{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word Embeddings\n",
        "\n",
        "In this notebook, we'll put into practice the concepts explored in the embedding directory:\n",
        "\n",
        "- [Bag-of-Words (BoW)](./bow.md)\n",
        "- [TF-IDF](./tfidf.md)\n",
        "- [Word2Vec](./word2vec.md)\n",
        "- [nn.Embedding](./nnembedding.md)\n",
        "\n",
        "We will start from the simplest count-based methods and move towards learned dense embeddings. We'll use the brown corpus from NLTK at 10000 randomly-sampled sentences, with the subsampling thing we talked about in Word2Vec.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from nltk.corpus import brown\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab Size: 4244\n",
            "Number of training pairs: 72558\n",
            "Sample pairs: [('he', 'wanting'), ('wanting', 'he'), ('group', 'willing')]\n"
          ]
        }
      ],
      "source": [
        "text_data = [\" \".join(s) for s in random.sample(sorted(brown.sents()), 10000)]\n",
        "\n",
        "#  Count frequencies\n",
        "tokens = []\n",
        "for sentence in text_data:\n",
        "    tokens.extend(sentence.lower().split())\n",
        "\n",
        "word_counts = Counter(tokens)\n",
        "\n",
        "# drop super infrequent words\n",
        "min_count = 5\n",
        "tokens = [t for t in tokens if word_counts[t] >= min_count]\n",
        "\n",
        "# update these based on the filtered list\n",
        "word_counts = Counter(tokens)\n",
        "total_count = len(tokens)\n",
        "\n",
        "# NOW build the vocab\n",
        "vocab = {word: i for i, word in enumerate(set(tokens))}\n",
        "idx2word = {i: word for word, i in vocab.items()}\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "# Subsampling Threshold\n",
        "# standard values are usually between 1e-3 and 1e-5\n",
        "threshold = 1e-5\n",
        "\n",
        "\n",
        "def should_keep(word):\n",
        "    count = word_counts[word]\n",
        "    freq = count / total_count\n",
        "\n",
        "    # The Word2Vec formula for keeping a word\n",
        "    # P(keep) = sqrt(threshold / freq)\n",
        "    # (If freq is small, this is > 1, so we always keep it)\n",
        "    p_keep = math.sqrt(threshold / (freq + 1e-8))\n",
        "\n",
        "    return random.random() < p_keep\n",
        "\n",
        "\n",
        "# Generate skip-gram pairs\n",
        "WINDOW_SIZE = 5\n",
        "data = []\n",
        "\n",
        "for sentence in text_data:\n",
        "    sent_tokens = sentence.lower().split()\n",
        "\n",
        "    # Filter and close the gaps, so \"the cat sat\" might become \"cat sat\" if \"the\" is dropped\n",
        "    subsampled_tokens = [w for w in sent_tokens if w in vocab and should_keep(w)]\n",
        "\n",
        "    # Generate pairs from the CLEANED list\n",
        "    for i, word in enumerate(subsampled_tokens):\n",
        "        center_idx = vocab[word]\n",
        "\n",
        "        for w in range(-WINDOW_SIZE, WINDOW_SIZE + 1):\n",
        "            context_pos = i + w\n",
        "            # bounds check on the NEW length\n",
        "            if w != 0 and 0 <= context_pos < len(subsampled_tokens):\n",
        "                context_word = subsampled_tokens[context_pos]\n",
        "                context_idx = vocab[context_word]\n",
        "                data.append((center_idx, context_idx))\n",
        "\n",
        "print(f\"Vocab Size: {VOCAB_SIZE}\")\n",
        "print(f\"Number of training pairs: {len(data)}\")\n",
        "print(f\"Sample pairs: {[(idx2word[c], idx2word[t]) for c, t in data[:3]]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Bag-of-Words (BoW)\n",
        "\n",
        "As discussed in [bow.md](./bow.md), BoW represents text as a fixed-length vector of word counts. It ignores grammar and word order.\n",
        "\n",
        "Let's implement a simple BoW vectorizer from scratch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BoW Vectors:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "class BoWVectorizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.idx2word = {}\n",
        "\n",
        "    def build_vocab(\n",
        "        self, corpus\n",
        "    ):  # A rlly simple vocab builder, but since we alrd did this up there, we skip it\n",
        "        # Tokenize and build vocabulary\n",
        "        unique_words = set()\n",
        "        for doc in corpus:\n",
        "            # Simple tokenization: lowercase and split by space\n",
        "            tokens = doc.lower().split()\n",
        "            unique_words.update(tokens)\n",
        "\n",
        "        # Assign indices\n",
        "        self.vocab = {word: i for i, word in enumerate(sorted(list(unique_words)))}\n",
        "        self.idx2word = {i: word for word, i in self.vocab.items()}\n",
        "        print(f\"Vocabulary Size: {len(self.vocab)}\")\n",
        "        print(f\"Vocab: {self.vocab}\")\n",
        "\n",
        "    def transform(self, corpus):\n",
        "        vectors = []\n",
        "        for doc in corpus:\n",
        "            tokens = doc.lower().split()\n",
        "            # Create a vector of zeros\n",
        "            vec = [0] * len(self.vocab)\n",
        "\n",
        "            # Count frequencies\n",
        "            for token in tokens:\n",
        "                if token in self.vocab:\n",
        "                    idx = self.vocab[token]\n",
        "                    vec[idx] += 1\n",
        "            vectors.append(vec)\n",
        "        return np.array(vectors)\n",
        "\n",
        "\n",
        "bow = BoWVectorizer()\n",
        "bow.vocab = vocab\n",
        "bow.idx2word = idx2word\n",
        "bow_vectors = bow.transform(text_data)\n",
        "\n",
        "print(\"\\nBoW Vectors:\")\n",
        "print(bow_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. TF-IDF\n",
        "\n",
        "BoW gives too much weight to common words like \"the\". [TF-IDF](./tfidf.md) addresses this by re-weighting counts based on how rare a word is across documents.\n",
        "\n",
        "$$ \\text{tfidf}(t, d) = \\text{tf}(t,d)\\times \\text{idf}(t) $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TF-IDF Vectors:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "class TFIDFVectorizer(BoWVectorizer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # keeping idf as an array aligned with vocab indices is way faster than a dict\n",
        "        self.idf = None\n",
        "\n",
        "    def fit(self, corpus):\n",
        "        if not (self.idx2word or self.vocab):\n",
        "            self.build_vocab(corpus)\n",
        "        N = len(corpus)\n",
        "\n",
        "        # get the raw counts first (n_docs, n_vocab)\n",
        "        # this assumes super().transform() returns a numpy array\n",
        "        tf_matrix = super().transform(corpus)\n",
        "\n",
        "        # calculate doc freq (df) by checking where count > 0\n",
        "        # axis=0 collapses the rows (docs), giving us count per word\n",
        "        df = np.sum(tf_matrix > 0, axis=0)\n",
        "\n",
        "        # vectorised idf calc. added small epsilon to avoid div by zero\n",
        "        self.idf = np.log(N / (df + 1e-9))\n",
        "\n",
        "    def transform(self, corpus):\n",
        "        tf_vectors = super().transform(corpus)\n",
        "\n",
        "        # numpy broadcasting automatically multiplies the idf vector\n",
        "        return tf_vectors * self.idf\n",
        "\n",
        "\n",
        "tfidf = TFIDFVectorizer()\n",
        "tfidf.vocab = vocab\n",
        "tfidf.idx2word = idx2word\n",
        "tfidf.fit(text_data)\n",
        "tfidf_vectors = tfidf.transform(text_data)\n",
        "\n",
        "print(\"\\nTF-IDF Vectors:\")\n",
        "print(np.round(tfidf_vectors, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Word2Vec (Skip-Gram)\n",
        "\n",
        "Moving to dense embeddings. As described in [word2vec.md](./word2vec.md), we'll implement an SGNS model. The goal is to predict context words given a center word. We will implement this using PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Word2Vec (SGNS)...\n",
            "Unigram weights ready.\n",
            "\n",
            "Training with Unigram Negatives...\n",
            "Epoch 0, Loss: 5.6791\n",
            "Epoch 50, Loss: 0.0380\n",
            "Epoch 100, Loss: 0.0306\n",
            "Epoch 150, Loss: 0.0309\n",
            "Epoch 200, Loss: 0.0322\n",
            "\n",
            "Learned Embeddings (first 3 words):\n",
            "Word Index 0: [ 0.58  0.05  1.22 -1.68 -0.19 -0.02  0.24  0.31  0.83  0.33  0.96 -0.72\n",
            " -0.54 -1.77 -1.25 -1.06  0.43  1.4  -0.41  0.04  0.03  0.01  0.3   1.18\n",
            "  0.62  0.12  0.39  2.5   1.1  -0.74 -0.47  0.01 -0.71 -1.    0.53  0.62\n",
            " -0.68 -0.53  0.92  0.94 -1.32  1.47 -0.55  0.5  -0.72  0.38 -0.93 -1.65\n",
            " -0.81  0.21 -1.18  0.42 -0.75  0.28  0.5   0.43  0.19  0.74 -0.88  0.7\n",
            "  1.14  0.61  0.18  0.74 -0.04 -0.22  0.17  0.31  0.39  1.19 -0.92  0.39\n",
            "  0.93  0.5  -2.05  1.23  1.44  0.82 -0.39 -0.93  0.18  0.39 -1.34 -2.14\n",
            "  0.8   0.33  0.33 -0.18  0.64 -1.66 -0.75 -0.31 -0.43 -0.71  1.43  0.03\n",
            " -0.62  0.4   0.79  0.21  0.37 -0.55 -0.23  0.13  0.09  1.06 -0.27  0.32\n",
            "  0.94 -0.39  0.24  0.47 -1.31 -1.28 -2.79  0.76  0.6   0.38 -0.95  1.28\n",
            "  0.07  1.48 -0.46 -0.16  0.46 -0.09  1.03  0.41  0.03 -0.68 -0.64 -1.23\n",
            "  1.49  0.87 -0.75 -0.46  0.59 -0.48 -1.29 -0.1   0.79  1.94 -0.14 -0.4\n",
            " -0.05  0.49  0.52  1.19 -0.72  0.07 -1.18  0.6   1.28 -0.19 -0.39  0.4\n",
            " -1.63  0.14 -0.16  0.27  0.36  0.28 -1.1   0.97  1.53  0.28  0.06  0.08\n",
            " -2.03 -0.48  0.63 -0.39  0.28  0.92 -0.56  2.27  0.13 -0.07 -0.42 -0.7\n",
            " -1.12 -0.55 -0.35  0.12 -0.42 -0.69 -0.54 -0.63 -1.1   0.12 -0.54  0.35\n",
            "  1.45  0.32  0.53 -1.02 -0.87  0.14 -0.54 -0.05]\n",
            "Word Index 1: [-0.67  0.45  0.4  -0.85  1.14  0.53  0.6   1.02 -0.77  0.2   0.08 -0.35\n",
            " -0.17 -0.39 -1.27 -1.09 -0.9   1.98 -1.    0.58 -0.36  0.14 -0.19 -0.26\n",
            "  0.57 -0.62 -0.06 -0.54  0.73  0.41  0.06  0.09  0.47  0.24 -0.27 -1.79\n",
            " -0.82  0.32  1.11  0.92 -1.02  0.44 -0.92  1.31 -0.53  0.   -0.95 -0.25\n",
            "  0.18  0.21 -0.12  1.03 -1.01 -0.18 -0.12 -1.33  0.42  0.78 -2.62 -0.15\n",
            " -0.63  2.44 -0.21  0.26 -0.69  0.11  0.6   1.48 -0.02  0.63 -0.46 -0.17\n",
            " -0.13  0.6  -1.83  0.71 -0.41  0.72  0.21 -0.53 -1.47 -0.16  0.1   0.45\n",
            " -1.26  0.33  0.23 -0.48  0.78 -1.4  -0.09  0.54  0.04 -1.38  0.06 -0.43\n",
            "  0.73 -0.24  1.51  0.2  -0.83 -0.4   0.46  0.3  -0.71 -1.5   0.01  0.55\n",
            " -0.73  0.23 -0.09 -0.63 -1.3  -1.56 -1.32 -0.07  0.02 -0.21  0.3   0.3\n",
            "  0.33  0.08 -0.76 -0.24  1.69 -0.51  0.67 -0.62 -0.51  0.17 -0.28 -0.5\n",
            "  0.12 -0.15 -0.38 -1.   -0.62  0.39 -0.9   0.82  1.25  0.21 -0.4  -0.02\n",
            " -0.77  0.19  0.73 -0.05 -0.17 -1.37  1.23  0.36  0.22  0.63 -0.06 -1.05\n",
            " -0.74  0.41 -0.84  0.19  1.73 -0.45  0.51  0.46 -0.1  -0.85 -0.02  0.98\n",
            "  0.25 -1.06  0.4  -0.07  1.22 -0.24 -0.19  0.16  1.18 -0.01  0.38  0.31\n",
            " -0.78  0.64 -0.17  0.23 -0.67 -0.41  0.15 -1.07  0.31 -0.47 -2.    0.19\n",
            " -0.39 -1.16  0.1   0.5   0.22 -0.46 -0.11  1.32]\n",
            "Word Index 2: [-0.29  0.78  0.95 -0.9   0.81 -0.11 -0.06  2.04  0.74 -1.71  0.09 -0.03\n",
            "  0.54 -0.46 -0.28 -0.02  1.04  0.32 -0.1   0.94 -0.43 -1.77 -0.29 -0.21\n",
            " -0.77 -0.32 -0.31 -0.42  0.64  0.29  0.83  0.18 -0.49  1.02 -0.09  0.69\n",
            " -0.   -0.06  0.55  1.14 -1.49 -0.18 -1.69  0.21 -1.35  2.38 -0.95 -0.36\n",
            " -0.41  0.95 -0.99  0.85 -1.53 -0.42  1.41  0.63  2.21 -0.68  0.44  0.93\n",
            "  0.82  1.34  0.27  0.75  0.5  -1.04 -0.03 -0.04 -0.11  0.62 -0.98 -1.12\n",
            "  0.35 -0.43 -1.21  0.01  1.19  1.33  1.1  -1.26 -0.25 -1.09  0.08 -0.25\n",
            "  0.08 -1.21 -0.93 -0.92  0.14 -0.09 -0.31 -0.39  0.07  0.95  0.61 -0.12\n",
            "  1.44  0.83 -0.45 -0.11 -0.13 -0.09 -0.08  1.25  0.72 -2.39 -0.4   1.01\n",
            "  0.08 -0.35  0.77  0.17 -1.53 -0.11  0.38  0.17 -0.05 -0.21 -0.77 -0.15\n",
            "  0.01  0.47 -0.32 -0.39 -0.2   0.05 -0.37 -0.36  1.   -0.25  0.1  -0.41\n",
            "  1.62 -1.32 -0.65 -1.52  0.8  -0.19 -0.43  0.93  1.32  1.52  0.93 -1.04\n",
            " -0.58  0.68 -0.36  0.19  0.87 -1.78  0.01  0.68  2.36 -2.14  0.58  1.19\n",
            " -1.35  0.79 -0.98 -0.67  1.68  0.82  0.75  0.14  1.02 -0.26 -0.21  0.48\n",
            " -0.49  1.59  0.84  0.74  2.04 -1.28  0.79  0.93  0.09 -0.71 -0.1  -0.5\n",
            " -2.26  0.61 -0.3   0.94 -0.61 -0.14  0.32 -0.4  -1.01  0.71 -0.9   0.3\n",
            "  0.54  0.76 -0.03 -0.59 -1.82  0.19  1.65  0.34]\n"
          ]
        }
      ],
      "source": [
        "# 1. Define Model (SGNS)\n",
        "class SkipGramNSModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        # Two embeddings: one for when the word is 'center', one for 'context'\n",
        "        self.center_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    def forward(self, center_idxs, context_idxs):\n",
        "        # center_idxs: [batch_size]\n",
        "        # context_idxs: [batch_size]\n",
        "\n",
        "        center_embeds = self.center_embeddings(center_idxs)  # [batch, dim]\n",
        "        context_embeds = self.context_embeddings(context_idxs)  # [batch, dim]\n",
        "\n",
        "        # Dot product between center and context vectors\n",
        "        # Result shape: [batch]\n",
        "        scores = torch.sum(center_embeds * context_embeds, dim=1)\n",
        "        return scores\n",
        "\n",
        "\n",
        "# Configuration\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_DIM = 200\n",
        "NUM_NEGATIVES = 5  # Number of negative samples per positive pair\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SkipGramNSModel(VOCAB_SIZE, EMBED_DIM).to(device)\n",
        "\n",
        "# SGNS uses Binary Cross Entropy (with Logits for stability)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "# 3. Train\n",
        "print(\"\\nTraining Word2Vec (SGNS)...\")\n",
        "\n",
        "# 1. Calculate Unigram Distribution\n",
        "# flatten the data to count word occurrences\n",
        "all_indices = [idx for pair in data for idx in pair]\n",
        "counts = Counter(all_indices)\n",
        "\n",
        "# create a frequency tensor sorted by vocab index\n",
        "freqs = torch.zeros(VOCAB_SIZE)\n",
        "for idx in range(VOCAB_SIZE):\n",
        "    freqs[idx] = counts.get(idx, 1)  # default to 1 to avoid zero division/errors\n",
        "\n",
        "# apply the magic 3/4 power\n",
        "unigram_weights = freqs.pow(0.75)\n",
        "# normalize so it sums to 1 (probabilities)\n",
        "unigram_weights = unigram_weights / unigram_weights.sum()\n",
        "\n",
        "print(\"Unigram weights ready.\")\n",
        "print(\"\\nTraining with Unigram Negatives...\")\n",
        "for epoch in range(201):\n",
        "    random.shuffle(data)\n",
        "\n",
        "    # positive samples\n",
        "    pos_centers = torch.LongTensor([p[0] for p in data]).to(device)\n",
        "    pos_contexts = torch.LongTensor([p[1] for p in data]).to(device)\n",
        "    pos_labels = torch.ones(len(data)).to(device)\n",
        "\n",
        "    # negative samples\n",
        "    neg_centers = pos_centers.repeat(NUM_NEGATIVES)\n",
        "\n",
        "    # sample from our calculated weights instead of uniform random\n",
        "    # we need (batch_size * num_negatives) samples\n",
        "    num_neg_samples = len(data) * NUM_NEGATIVES\n",
        "\n",
        "    # torch.multinomial samples indices based on the probability distribution we made\n",
        "    neg_contexts = torch.multinomial(\n",
        "        unigram_weights, num_neg_samples, replacement=True\n",
        "    ).to(device)\n",
        "\n",
        "    neg_labels = torch.zeros(num_neg_samples).to(device)\n",
        "\n",
        "    all_centers = torch.cat([pos_centers, neg_centers])\n",
        "    all_contexts = torch.cat([pos_contexts, neg_contexts])\n",
        "    all_labels = torch.cat([pos_labels, neg_labels])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(all_centers, all_contexts)\n",
        "    loss = criterion(output, all_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 4. Inspect Embeddings\n",
        "print(\"\\nLearned Embeddings (first 3 words):\")\n",
        "with torch.no_grad():\n",
        "    for i in range(3):\n",
        "        vec = model.center_embeddings.weight[i]\n",
        "        print(f\"Word Index {i}: {vec.cpu().numpy().round(2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. nn.Embedding & Hashed Embedding\n",
        "\n",
        "As mentioned in [nnembedding.md](./nnembedding.md), standard `nn.Embedding` is just a lookup table.\n",
        "\n",
        "Here, we'll try implementing the hash embedding concept we went over as the optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram weights ready.\n",
            "\n",
            "Training with Unigram Negatives...\n",
            "Epoch 0, Loss: 3.2161\n",
            "Epoch 50, Loss: 0.3715\n",
            "Epoch 100, Loss: 0.3179\n",
            "Epoch 150, Loss: 0.2885\n",
            "Epoch 200, Loss: 0.2781\n",
            "\n",
            "Learned Hashed Embeddings (SGNS):\n",
            "Word 0: [ 0.58  0.05  1.22 -1.68 -0.19 -0.02  0.24  0.31  0.83  0.33  0.96 -0.72\n",
            " -0.54 -1.77 -1.25 -1.06  0.43  1.4  -0.41  0.04  0.03  0.01  0.3   1.18\n",
            "  0.62  0.12  0.39  2.5   1.1  -0.74 -0.47  0.01 -0.71 -1.    0.53  0.62\n",
            " -0.68 -0.53  0.92  0.94 -1.32  1.47 -0.55  0.5  -0.72  0.38 -0.93 -1.65\n",
            " -0.81  0.21 -1.18  0.42 -0.75  0.28  0.5   0.43  0.19  0.74 -0.88  0.7\n",
            "  1.14  0.61  0.18  0.74 -0.04 -0.22  0.17  0.31  0.39  1.19 -0.92  0.39\n",
            "  0.93  0.5  -2.05  1.23  1.44  0.82 -0.39 -0.93  0.18  0.39 -1.34 -2.14\n",
            "  0.8   0.33  0.33 -0.18  0.64 -1.66 -0.75 -0.31 -0.43 -0.71  1.43  0.03\n",
            " -0.62  0.4   0.79  0.21  0.37 -0.55 -0.23  0.13  0.09  1.06 -0.27  0.32\n",
            "  0.94 -0.39  0.24  0.47 -1.31 -1.28 -2.79  0.76  0.6   0.38 -0.95  1.28\n",
            "  0.07  1.48 -0.46 -0.16  0.46 -0.09  1.03  0.41  0.03 -0.68 -0.64 -1.23\n",
            "  1.49  0.87 -0.75 -0.46  0.59 -0.48 -1.29 -0.1   0.79  1.94 -0.14 -0.4\n",
            " -0.05  0.49  0.52  1.19 -0.72  0.07 -1.18  0.6   1.28 -0.19 -0.39  0.4\n",
            " -1.63  0.14 -0.16  0.27  0.36  0.28 -1.1   0.97  1.53  0.28  0.06  0.08\n",
            " -2.03 -0.48  0.63 -0.39  0.28  0.92 -0.56  2.27  0.13 -0.07 -0.42 -0.7\n",
            " -1.12 -0.55 -0.35  0.12 -0.42 -0.69 -0.54 -0.63 -1.1   0.12 -0.54  0.35\n",
            "  1.45  0.32  0.53 -1.02 -0.87  0.14 -0.54 -0.05]\n",
            "Word 1: [-0.67  0.45  0.4  -0.85  1.14  0.53  0.6   1.02 -0.77  0.2   0.08 -0.35\n",
            " -0.17 -0.39 -1.27 -1.09 -0.9   1.98 -1.    0.58 -0.36  0.14 -0.19 -0.26\n",
            "  0.57 -0.62 -0.06 -0.54  0.73  0.41  0.06  0.09  0.47  0.24 -0.27 -1.79\n",
            " -0.82  0.32  1.11  0.92 -1.02  0.44 -0.92  1.31 -0.53  0.   -0.95 -0.25\n",
            "  0.18  0.21 -0.12  1.03 -1.01 -0.18 -0.12 -1.33  0.42  0.78 -2.62 -0.15\n",
            " -0.63  2.44 -0.21  0.26 -0.69  0.11  0.6   1.48 -0.02  0.63 -0.46 -0.17\n",
            " -0.13  0.6  -1.83  0.71 -0.41  0.72  0.21 -0.53 -1.47 -0.16  0.1   0.45\n",
            " -1.26  0.33  0.23 -0.48  0.78 -1.4  -0.09  0.54  0.04 -1.38  0.06 -0.43\n",
            "  0.73 -0.24  1.51  0.2  -0.83 -0.4   0.46  0.3  -0.71 -1.5   0.01  0.55\n",
            " -0.73  0.23 -0.09 -0.63 -1.3  -1.56 -1.32 -0.07  0.02 -0.21  0.3   0.3\n",
            "  0.33  0.08 -0.76 -0.24  1.69 -0.51  0.67 -0.62 -0.51  0.17 -0.28 -0.5\n",
            "  0.12 -0.15 -0.38 -1.   -0.62  0.39 -0.9   0.82  1.25  0.21 -0.4  -0.02\n",
            " -0.77  0.19  0.73 -0.05 -0.17 -1.37  1.23  0.36  0.22  0.63 -0.06 -1.05\n",
            " -0.74  0.41 -0.84  0.19  1.73 -0.45  0.51  0.46 -0.1  -0.85 -0.02  0.98\n",
            "  0.25 -1.06  0.4  -0.07  1.22 -0.24 -0.19  0.16  1.18 -0.01  0.38  0.31\n",
            " -0.78  0.64 -0.17  0.23 -0.67 -0.41  0.15 -1.07  0.31 -0.47 -2.    0.19\n",
            " -0.39 -1.16  0.1   0.5   0.22 -0.46 -0.11  1.32]\n",
            "Word 2: [-0.29  0.78  0.95 -0.9   0.81 -0.11 -0.06  2.04  0.74 -1.71  0.09 -0.03\n",
            "  0.54 -0.46 -0.28 -0.02  1.04  0.32 -0.1   0.94 -0.43 -1.77 -0.29 -0.21\n",
            " -0.77 -0.32 -0.31 -0.42  0.64  0.29  0.83  0.18 -0.49  1.02 -0.09  0.69\n",
            " -0.   -0.06  0.55  1.14 -1.49 -0.18 -1.69  0.21 -1.35  2.38 -0.95 -0.36\n",
            " -0.41  0.95 -0.99  0.85 -1.53 -0.42  1.41  0.63  2.21 -0.68  0.44  0.93\n",
            "  0.82  1.34  0.27  0.75  0.5  -1.04 -0.03 -0.04 -0.11  0.62 -0.98 -1.12\n",
            "  0.35 -0.43 -1.21  0.01  1.19  1.33  1.1  -1.26 -0.25 -1.09  0.08 -0.25\n",
            "  0.08 -1.21 -0.93 -0.92  0.14 -0.09 -0.31 -0.39  0.07  0.95  0.61 -0.12\n",
            "  1.44  0.83 -0.45 -0.11 -0.13 -0.09 -0.08  1.25  0.72 -2.39 -0.4   1.01\n",
            "  0.08 -0.35  0.77  0.17 -1.53 -0.11  0.38  0.17 -0.05 -0.21 -0.77 -0.15\n",
            "  0.01  0.47 -0.32 -0.39 -0.2   0.05 -0.37 -0.36  1.   -0.25  0.1  -0.41\n",
            "  1.62 -1.32 -0.65 -1.52  0.8  -0.19 -0.43  0.93  1.32  1.52  0.93 -1.04\n",
            " -0.58  0.68 -0.36  0.19  0.87 -1.78  0.01  0.68  2.36 -2.14  0.58  1.19\n",
            " -1.35  0.79 -0.98 -0.67  1.68  0.82  0.75  0.14  1.02 -0.26 -0.21  0.48\n",
            " -0.49  1.59  0.84  0.74  2.04 -1.28  0.79  0.93  0.09 -0.71 -0.1  -0.5\n",
            " -2.26  0.61 -0.3   0.94 -0.61 -0.14  0.32 -0.4  -1.01  0.71 -0.9   0.3\n",
            "  0.54  0.76 -0.03 -0.59 -1.82  0.19  1.65  0.34]\n"
          ]
        }
      ],
      "source": [
        "class HashedEmbedding(nn.Module):\n",
        "    def __init__(self, num_buckets, embed_dim, num_hashes=2):\n",
        "        super().__init__()\n",
        "        self.num_buckets = num_buckets\n",
        "        self.num_hashes = num_hashes\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.table = nn.Embedding(num_buckets, embed_dim)\n",
        "        self.importance = nn.Embedding(num_buckets, 1)\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"primes\", torch.tensor([2654435761, 2246822507])[:num_hashes]\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"weight_primes\", torch.tensor([387420489, 982451653])[:num_hashes]\n",
        "        )\n",
        "\n",
        "    def forward(self, indices):\n",
        "        batch_sum = torch.zeros(indices.shape[0], self.embed_dim, device=indices.device)\n",
        "        for i in range(self.num_hashes):\n",
        "            hashed_idx = (indices * self.primes[i] + i) % self.num_buckets\n",
        "            weight_idx = (indices * self.weight_primes[i] + i) % self.num_buckets\n",
        "            vec = self.table(hashed_idx)\n",
        "            p_w = torch.sigmoid(self.importance(weight_idx))\n",
        "            batch_sum += vec * p_w\n",
        "        return batch_sum\n",
        "\n",
        "\n",
        "# Define the Hashed SGNS Model\n",
        "class HashedSkipGramNSModel(nn.Module):\n",
        "    def __init__(self, vocab_size, num_buckets, embed_dim):\n",
        "        super().__init__()\n",
        "        # instead of allocating VOCAB_SIZE vectors, we allocate NUM_BUCKETS\n",
        "        # for both center and context parts\n",
        "        self.center_embeddings = HashedEmbedding(num_buckets, embed_dim)\n",
        "        self.context_embeddings = HashedEmbedding(num_buckets, embed_dim)\n",
        "\n",
        "    def forward(self, center_idxs, context_idxs):\n",
        "        # the HashedEmbedding.forward() returns a vector [batch, dim]\n",
        "        # just like nn.Embedding does\n",
        "        center_embeds = self.center_embeddings(center_idxs)\n",
        "        context_embeds = self.context_embeddings(context_idxs)\n",
        "\n",
        "        # dot product\n",
        "        scores = torch.sum(center_embeds * context_embeds, dim=1)\n",
        "        return scores\n",
        "\n",
        "\n",
        "# Configuration\n",
        "# lets assume vocab is huge but we only want small memory usage\n",
        "VOCAB_SIZE = len(vocab)\n",
        "PHYSICAL_BUCKETS = VOCAB_SIZE // 8  # effectively compresses the vocab\n",
        "EMBED_DIM = 200\n",
        "NUM_NEGATIVES = 5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# init the new model\n",
        "hashed_model = HashedSkipGramNSModel(VOCAB_SIZE, PHYSICAL_BUCKETS, EMBED_DIM).to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(hashed_model.parameters(), lr=0.01)\n",
        "\n",
        "# Calculate Unigram Distribution\n",
        "# flatten the data to count word occurrences\n",
        "all_indices = [idx for pair in data for idx in pair]\n",
        "counts = Counter(all_indices)\n",
        "\n",
        "# create a frequency tensor sorted by vocab index\n",
        "freqs = torch.zeros(VOCAB_SIZE)\n",
        "for idx in range(VOCAB_SIZE):\n",
        "    freqs[idx] = counts.get(idx, 1)  # default to 1 to avoid zero division/errors\n",
        "\n",
        "# apply the magic 3/4 power\n",
        "unigram_weights = freqs.pow(0.75)\n",
        "# normalize so it sums to 1 (probabilities)\n",
        "unigram_weights = unigram_weights / unigram_weights.sum()\n",
        "\n",
        "print(\"Unigram weights ready.\")\n",
        "optimizer = optim.Adam(hashed_model.parameters(), lr=0.1)\n",
        "\n",
        "print(\"\\nTraining with Unigram Negatives...\")\n",
        "for epoch in range(201):\n",
        "    random.shuffle(data)\n",
        "\n",
        "    # positive samples\n",
        "    pos_centers = torch.LongTensor([p[0] for p in data]).to(device)\n",
        "    pos_contexts = torch.LongTensor([p[1] for p in data]).to(device)\n",
        "    pos_labels = torch.ones(len(data)).to(device)\n",
        "\n",
        "    # negative samples\n",
        "    # repeat centers to match num_negatives\n",
        "    neg_centers = pos_centers.repeat(NUM_NEGATIVES)\n",
        "\n",
        "    # sample from our calculated weights instead of uniform random\n",
        "    # we need (batch_size * num_negatives) samples\n",
        "    num_neg_samples = len(data) * NUM_NEGATIVES\n",
        "\n",
        "    # torch.multinomial samples indices based on the probability distribution we made\n",
        "    neg_contexts = torch.multinomial(\n",
        "        unigram_weights, num_neg_samples, replacement=True\n",
        "    ).to(device)\n",
        "\n",
        "    neg_labels = torch.zeros(num_neg_samples).to(device)\n",
        "\n",
        "    all_centers = torch.cat([pos_centers, neg_centers])\n",
        "    all_contexts = torch.cat([pos_contexts, neg_contexts])\n",
        "    all_labels = torch.cat([pos_labels, neg_labels])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = hashed_model(all_centers, all_contexts)\n",
        "    loss = criterion(output, all_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"\\nLearned Hashed Embeddings (SGNS):\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(3):  # print first 3 words\n",
        "\n",
        "        # make a tensor for the index (shape [1])\n",
        "        idx_tensor = torch.tensor([i], dtype=torch.long).to(device)\n",
        "        vec = model.center_embeddings(idx_tensor)\n",
        "\n",
        "        print(f\"Word {i}: {vec.squeeze().cpu().numpy().round(2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing the embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Comparing Embedding Spaces ---\n",
            "Pair                 | BoW      | TF-IDF   | SkipGram | Hashed  \n",
            "-----------------------------------------------------------------\n",
            "paris-france         | 0.000    | 0.000    | 0.129    | 0.088\n",
            "king-queen          | 0.000    | 0.000    | 0.182    | 0.095\n",
            "king-government     | 0.000    | 0.000    | 0.214    | 0.129\n",
            "dog-government     | 0.000    | 0.000    | 0.194    | 0.155\n"
          ]
        }
      ],
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# helper function cuz scipy cosine gives \"distance\" (0 is identical)\n",
        "def get_similarity(vec1, vec2):\n",
        "    if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
        "        return 0.0\n",
        "    return 1 - cosine(vec1, vec2)\n",
        "\n",
        "\n",
        "pairs = [\n",
        "    (\"paris\", \"france\"),\n",
        "    (\"king\", \"queen\"),\n",
        "    (\"king\", \"government\"),\n",
        "    (\"dog\", \"government\"),\n",
        "]\n",
        "\n",
        "# filter pairs to make sure they exist in our vocab\n",
        "valid_pairs = []\n",
        "for w1, w2 in pairs:\n",
        "    if w1 in vocab and w2 in vocab:\n",
        "        valid_pairs.append((w1, w2))\n",
        "    else:\n",
        "        print(f\"skipping ({w1}, {w2}) - not in vocab\")\n",
        "\n",
        "print(f\"\\n--- Comparing Embedding Spaces ---\")\n",
        "print(f\"{'Pair':<20} | {'BoW':<8} | {'TF-IDF':<8} | {'SkipGram':<8} | {'Hashed':<8}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for w1, w2 in valid_pairs:\n",
        "    idx1, idx2 = vocab[w1], vocab[w2]\n",
        "\n",
        "    v_bow1 = bow_vectors[:, idx1]\n",
        "    v_bow2 = bow_vectors[:, idx2]\n",
        "    sim_bow = get_similarity(v_bow1, v_bow2)\n",
        "\n",
        "    v_tfidf1 = tfidf_vectors[:, idx1]\n",
        "    v_tfidf2 = tfidf_vectors[:, idx2]\n",
        "    sim_tfidf = get_similarity(v_tfidf1, v_tfidf2)\n",
        "\n",
        "    # Standard SkipGram\n",
        "    with torch.no_grad():\n",
        "        v_sg1 = model.center_embeddings(torch.tensor(idx1).to(device)).cpu().numpy()\n",
        "        v_sg2 = model.center_embeddings(torch.tensor(idx2).to(device)).cpu().numpy()\n",
        "    sim_sg = get_similarity(v_sg1, v_sg2)\n",
        "\n",
        "    # Hashed SkipGram\n",
        "    with torch.no_grad():\n",
        "        # we gotta unsqueeze inputs cuz your hashed embedding expects batches\n",
        "        v_hash1 = (\n",
        "            hashed_model.center_embeddings(torch.tensor([idx1]).to(device))\n",
        "            .cpu()\n",
        "            .squeeze()\n",
        "            .numpy()\n",
        "        )\n",
        "        v_hash2 = (\n",
        "            hashed_model.center_embeddings(torch.tensor([idx2]).to(device))\n",
        "            .cpu()\n",
        "            .squeeze()\n",
        "            .numpy()\n",
        "        )\n",
        "    sim_hash = get_similarity(v_hash1, v_hash2)\n",
        "\n",
        "    print(\n",
        "        f\"{w1}-{w2:<14} | {sim_bow:.3f}    | {sim_tfidf:.3f}    | {sim_sg:.3f}    | {sim_hash:.3f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, there's definitely some level of fidelity loss to the embedding quality when you reduce the number of \"actual\" vocabulary, but when you scale up your training, this problem eventually amortizes.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
