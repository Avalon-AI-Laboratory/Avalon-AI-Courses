{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß± Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, characters, or subwords. Check out the [preamble](./README.md)\n",
    "for what tokenization does, why, and how. The material list of this is as follows, sorted by how you're supposed to read them:\n",
    "\n",
    "> .  \n",
    "> ‚îú‚îÄ‚îÄ [README.md](./README.md)  \n",
    "> ‚îú‚îÄ‚îÄ [rulebased.md](./rulebased.md)  \n",
    "> ‚îú‚îÄ‚îÄ [bpe.md](./bpe.md)  \n",
    "> ‚îú‚îÄ‚îÄ [unigram.md](./unigram.md)  \n",
    "> ‚îî‚îÄ‚îÄ [wordpiece.md](./wordpiece.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-Based Tokenization\n",
    "\n",
    "Rule-based tokenization uses a set of predefined rules to split text into tokens. We can use `nltk` for this via its `word_tokenize` thing, which is derived from Penn Treebank's tokenizer. It's a bit advanced, so you can check out how it works [yourself](https://github.com/nltk/nltk/blob/5e0a6c4d69f001d86e86ada9b9f9a363bae1c692/nltk/tokenize/destructive.py#L37), but in essence, it's basically splitting off quotes, then punctuations, parentheses, dashes, then splitting off contracted words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem', 'ipsum', 'dolor', 'sit', 'amet', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Lorem ipsum dolor sit amet.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE) Tokenization\n",
    "\n",
    "BPE is the subword tokenization algorithm used in models like GPT-2 and RoBERTa. This bit isn't explained in the [notes](./bpe.md), but `ƒ†` is used to pad when a token begins with space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'orem', 'ƒ†', 'ips', 'um', 'ƒ†d', 'olor', 'ƒ†sit', 'ƒ†am', 'et', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "text = \"Lorem ipsum dolor sit amet.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Tokenization\n",
    "\n",
    "Unigram is the subword tokenization algorithm used in models like XLNet. You can check out the explainer in the [notes](./unigram.md) for how the logic works. Those `\"_\"`s in the output is caused by how SentencePiece intializes the initial character vocab, you can assume them as just spaces. [^1]\n",
    "\n",
    "[^1]: https://huggingface.co/learn/llm-course/en/chapter6/7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅLore', 'm', '‚ñÅ', 'ip', 'sum', '‚ñÅdo', 'lor', '‚ñÅsit', '‚ñÅa', 'met', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "text = \"Lorem ipsum dolor sit amet.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPiece Tokenization\n",
    "\n",
    "WordPiece is the subword tokenization algorithm used in models like BERT. Check out the related [notes](./wordpiece.md) to see how it works. As you can see, the tokenization adds these `#` paddings to allow for modelling of non-beginning tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lo', '##rem', 'i', '##ps', '##um', 'do', '##lor', 'sit', 'am', '##et', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "text = \"Lorem ipsum dolor sit amet.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
