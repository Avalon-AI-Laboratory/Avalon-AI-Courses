{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tokenization Challenge\n",
    "\n",
    "Welcome to the tokenization challenge! Just like the previous chapter on normalization, since you've finished all subchapters, you get to work on a challenge! <3\n",
    "\n",
    "In this notebook, you'll be tasked with choosing and applying the correct tokenization techniques to solve a series of problems. Each challenge will require you to implement a `my_tokenizer_func_*` that correctly tokenizes the input text for a simple classifier to work. If you succeed, you'll get a piece of a flag. At the end, you can check if you've collected all the pieces and assembled the flag correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**\n",
    "\n",
    "You can ignore this bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import hashlib\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "def binary_to_flag(bin_array):\n",
    "    \"\"\"Converts a numpy array of bits back to a string.\"\"\"\n",
    "    try:\n",
    "        bin_str = \"\".join(map(str, bin_array))\n",
    "        byte_chunks = [bin_str[i : i + 8] for i in range(0, len(bin_str), 8)]\n",
    "        chars = [chr(int(byte, 2)) for byte in byte_chunks]\n",
    "        return \"\".join(chars)\n",
    "    except Exception:\n",
    "        return \"failed :<\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Challenges**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Challenge A ---\n",
      "Running Your Model...\n",
      "Your Model Decoded Flag:   每每\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "#  CHALLENGE A\n",
    "# ==========================================================\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "print(\"---  Challenge A ---\")\n",
    "\n",
    "# --- 1. Challenge A Data ---\n",
    "train_texts_A = [\n",
    "    \"So he got kiwi,   apple, and    pear\",\n",
    "    \"He wanted banana and    pear\",\n",
    "    \"the banana  shouldn't be conflated with  orange\",\n",
    "    \"and while apple is similar to pear,\",\n",
    "    \"kiwi is not similar at all\",\n",
    "    \"The eggplant was girthy\",\n",
    "    \"tomato is red, while melons are green-ish\",\n",
    "    \"green too are grape,\",\n",
    "    \"   kiwi, and pear\",\n",
    "    \"three things: truffle, strawberry, gold\",\n",
    "    \"car drink petrol\",\n",
    "    \"kiwi, pear, and apple can be green\",\n",
    "]\n",
    "train_labels_A = [1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1]\n",
    "test_texts_A = [\n",
    "    \"while\",\n",
    "    \"pear\",\n",
    "    \"eggplant\",\n",
    "    \"apple\",\n",
    "    \"with\",\n",
    "    \"red\",\n",
    "    \"is\",\n",
    "    \"what\",\n",
    "    \"shouldn't\",\n",
    "    \"he\",\n",
    "    \"drink\",\n",
    "    \"while\",\n",
    "    \"in\",\n",
    "    \"the\",\n",
    "    \"green\",\n",
    "    \"car\",\n",
    "]\n",
    "\n",
    "\n",
    "# --- 2. Student's Task ---\n",
    "def my_tokenizer_func_A(text):\n",
    "    # Modify this function to tokenize the text\n",
    "    return [text]\n",
    "\n",
    "\n",
    "# --- 3. Checker Cell (Your Model) ---\n",
    "print(\"Running Your Model...\")\n",
    "try:\n",
    "    model_A = make_pipeline(\n",
    "        CountVectorizer(tokenizer=my_tokenizer_func_A),\n",
    "        SVC(random_state=42),  # DO NOT CHANGE THIS\n",
    "    )\n",
    "    model_A.fit(train_texts_A, train_labels_A)\n",
    "    preds_A = model_A.predict(test_texts_A)\n",
    "    flag_piece_A = binary_to_flag(preds_A)\n",
    "    print(f\"Your Model Decoded Flag:   {flag_piece_A}\")\n",
    "except Exception as e:\n",
    "    print(f\" ERROR: Your function failed to run. {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Challenge B ---\n",
      "Running Your Model...\n",
      "Your Model Decoded Flag:   每每每\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "#  CHALLENGE B\n",
    "# ==========================================================\n",
    "print(\"---  Challenge B ---\")\n",
    "\n",
    "# --- 1. Challenge B Data ---\n",
    "train_texts_B = [\n",
    "    \"skibidi toilet sikibibibi\",\n",
    "    \"tung tung tung sahur\",\n",
    "    \"hur hurtung tung sahur\",\n",
    "    \"bahlili singkongkong hitamtam\",\n",
    "]\n",
    "train_labels_B = [1, 0, 0, 1]\n",
    "test_texts_B = [\n",
    "    \"ung\",\n",
    "    \"ili\",\n",
    "    \"ung\",\n",
    "    \"hur\",\n",
    "    \"hur\",\n",
    "    \"hur\",\n",
    "    \"sing\",\n",
    "    \"ung\",\n",
    "    \"ung\",\n",
    "    \"b\",\n",
    "    \"ahl\",\n",
    "    \"ili\",\n",
    "    \"ung\",\n",
    "    \"hur\",\n",
    "    \"hur\",\n",
    "    \"dungdung\",\n",
    "    \"tutung\",\n",
    "    \"bahlil\",\n",
    "    \"dung\",\n",
    "    \"singsung\",\n",
    "    \"hur\",\n",
    "    \"\",\n",
    "    \"ung\",\n",
    "    \"bah\",\n",
    "]\n",
    "\n",
    "\n",
    "# --- 2. Student's Task ---\n",
    "def my_tokenizer_func_B(text):\n",
    "    # Modify this function to tokenize\n",
    "    return [text]\n",
    "\n",
    "\n",
    "# --- 3. Checker Cell (Your Model) ---\n",
    "print(\"Running Your Model...\")\n",
    "try:\n",
    "    model_B = make_pipeline(\n",
    "        CountVectorizer(tokenizer=my_tokenizer_func_B),\n",
    "        SVC(random_state=42),  # DON'T CHANGE THIS\n",
    "    )\n",
    "    model_B.fit(train_texts_B, train_labels_B)\n",
    "    preds_B = model_B.predict(test_texts_B)\n",
    "    flag_piece_B = binary_to_flag(preds_B)\n",
    "    print(f\"Your Model Decoded Flag:   {flag_piece_B}\")\n",
    "except Exception as e:\n",
    "    print(f\" ERROR: Your function failed to run. {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Challenge C ---\n",
      "Running Your Model...\n",
      "Your Model Decoded Flag:   \u0000\u0000\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "#  CHALLENGE C\n",
    "# ==========================================================\n",
    "print(\"---  Challenge C ---\")\n",
    "\n",
    "# --- 1. Challenge C Data ---\n",
    "train_texts_C = [\n",
    "    \"tokenization is important\",\n",
    "    \"words sub powerful token\",\n",
    "    \"##ization ##word is are\",\n",
    "    \"subword tokenizers are powerful\",\n",
    "    \"good better than from\",\n",
    "    \"a compute ##s data\",\n",
    "    \"important is good tokenization as tokenization\",\n",
    "    \"##ization good better\",\n",
    "    \"subwords are better than words\",\n",
    "    \"words a compute\",\n",
    "    \"##word is from\",\n",
    "    \"words are from subwords\",\n",
    "    \"are than\",\n",
    "    \"sub powerful ##s\",\n",
    "    \"is is is\",\n",
    "    \"token data\",\n",
    "    \"better better\",\n",
    "    \"words words words\",\n",
    "    \"are are are\",\n",
    "    \"powerful powerful\",\n",
    "]\n",
    "train_labels_C = [\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "]\n",
    "\n",
    "test_texts_C = [\n",
    "    \"words\",\n",
    "    \"##ization\",\n",
    "    \"sub\",\n",
    "    \"##word\",\n",
    "    \"powerful\",\n",
    "    \"is\",\n",
    "    \"are\",\n",
    "    \"good\",\n",
    "    \"token\",\n",
    "    \"better\",\n",
    "    \"than\",\n",
    "    \"from\",\n",
    "    \"a\",\n",
    "    \"compute\",\n",
    "    \"##s\",\n",
    "    \"data\",\n",
    "]\n",
    "\n",
    "\n",
    "# --- 2. Student's Task ---\n",
    "def my_tokenizer_func_C(text):\n",
    "    # Modify this function to tokenize\n",
    "    return [text]\n",
    "\n",
    "\n",
    "# --- 3. Checker Cell (Your Model) ---\n",
    "print(\"Running Your Model...\")\n",
    "try:\n",
    "    model_C = make_pipeline(\n",
    "        CountVectorizer(tokenizer=my_tokenizer_func_C), SVC(random_state=42)\n",
    "    )\n",
    "    model_C.fit(train_texts_C, train_labels_C)\n",
    "    preds_C = model_C.predict(test_texts_C)\n",
    "    flag_piece_C = binary_to_flag(preds_C)\n",
    "    print(f\"Your Model Decoded Flag:   {flag_piece_C}\")\n",
    "except Exception as e:\n",
    "    print(f\" ERROR: Your function failed to run. {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  FLAG CHECK ---\n",
      "FLAG YOU GOT: avalon{每每_每每每_\u0000\u0000}\n",
      "锔 hmm, not yet... try rechecking your answers! :< 锔\n"
     ]
    }
   ],
   "source": [
    "print(\"---  FLAG CHECK ---\")\n",
    "\n",
    "print(f\"FLAG YOU GOT: avalon{{{flag_piece_A}_{flag_piece_B}_{flag_piece_C}}}\")\n",
    "\n",
    "if (\n",
    "    hashlib.sha256(\n",
    "        f\"avalon{{{flag_piece_A}_{flag_piece_B}_{flag_piece_C}}}\".encode(\"utf8\")\n",
    "    ).hexdigest()\n",
    "    == \"78c7d7153c219b1cc272ef6a0abb8b2729fd8b645342bb329655b3dc48f2c4f2\"\n",
    "):\n",
    "    print(\" CONGRATS, YOU DID IT :3 \")\n",
    "else:\n",
    "    print(\"锔 hmm, not yet... try rechecking your answers! :< 锔\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
